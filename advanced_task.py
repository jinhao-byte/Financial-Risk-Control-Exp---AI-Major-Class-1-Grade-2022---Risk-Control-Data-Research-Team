import pandas as pd
import lightgbm as lgb
from sklearn.model_selection import train_test_split
from sklearn.metrics import (
    classification_report, roc_auc_score, confusion_matrix, ConfusionMatrixDisplay
)
from sklearn.preprocessing import LabelEncoder
import joblib
import matplotlib.pyplot as plt
import seaborn as sns
import os  # æ–°å¢ï¼šç”¨äºåˆ›å»ºç›®å½•

# 1. è¯»å–å¹¶åˆå¹¶æ•°æ®
df_tr = pd.read_csv('data/train_transaction.csv')
df_id = pd.read_csv('data/train_identity.csv')
df = df_tr.merge(df_id, on='TransactionID', how='left')

# 2. æ ‡ç­¾ä¸ç‰¹å¾åˆ†ç¦»
y = df['isFraud']
X = df.drop(['isFraud', 'TransactionID'], axis=1)

# 3. ç¼ºå¤±å€¼å¤„ç†
X = X.fillna(-999)

# 4. ç±»åˆ«ç‰¹å¾ç¼–ç ï¼ˆä¿®æ”¹ï¼šä¿å­˜ç¼–ç å™¨ï¼‰
cat_encoders = {}  # æ–°å¢ï¼šå­˜å‚¨ç±»åˆ«ç¼–ç å™¨
cat_cols = X.select_dtypes('object').columns
for col in cat_cols:
    le = LabelEncoder()
    X[col] = le.fit_transform(X[col].astype(str))
    cat_encoders[col] = le  # æ–°å¢ï¼šä¿å­˜ç¼–ç å™¨åˆ°å­—å…¸

# 5. æ•°æ®åˆ’åˆ†
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, stratify=y, random_state=42
)

# LightGBM

import lightgbm as lgb
model = lgb.LGBMClassifier(
    n_estimators=1000,
    learning_rate=0.05,
    num_leaves=64,
    colsample_bytree=0.7,
    subsample=0.7,
    random_state=42,
    class_weight='balanced'
)
model.fit(
    X_train, y_train,
    eval_set=[(X_test, y_test)],
    eval_metric='F1-score',
    early_stopping_rounds=100,
    callbacks=[
        early_stopping(stopping_rounds=50),
        log_evaluation(period=100)
    ]
)
y_pred = model.predict(X_test)


class FraudTransformer(nn.Module):
    def __init__(self, num_numerical_features, categorical_dims, d_model=64, nhead=4, num_layers=3):
        super().__init__()
        self.d_model = d_model

        # è¾“å…¥å±‚ï¼šæ•°å€¼ç‰¹å¾æŠ•å½± + ç±»åˆ«åµŒå…¥
        self.numerical_proj = nn.Linear(num_numerical_features, d_model)
        self.category_embs = nn.ModuleList([nn.Embedding(dim, d_model) for dim in categorical_dims])

        # åºåˆ—å»ºæ¨¡ï¼šä½ç½®ç¼–ç  + Transformerç¼–ç å±‚
        self.pos_encoding = nn.Parameter(torch.randn(5000, d_model))
        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, batch_first=True)
        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)

        # è¾“å‡ºå±‚ï¼šCLS token + åˆ†ç±»å™¨
        self.cls_token = nn.Parameter(torch.randn(1, 1, d_model))
        self.classifier = nn.Linear(d_model, 1)

    def forward(self, num_x, cat_x, mask=None):
        batch_size, seq_len = num_x.shape[0], num_x.shape[1]

        # æ•°å€¼ç‰¹å¾æŠ•å½±
        num_proj = self.numerical_proj(num_x)
        # ç±»åˆ«ç‰¹å¾åµŒå…¥
        cat_embs = []
        for i, emb in enumerate(self.category_embs):
            cat_embs.append(emb(cat_x[:, :, i]))
        cat_proj = torch.stack(cat_embs).sum(dim=0)
        # ç‰¹å¾ç›¸åŠ 
        x = num_proj + cat_proj

        # æ·»åŠ ä½ç½®ç¼–ç 
        x = x + self.pos_encoding[:seq_len, :].unsqueeze(0)
        # Transformerç¼–ç 
        x = self.transformer(x, src_key_padding_mask=mask)


        # æ‰©å±•[CLS]æ ‡è®°
        cls_tokens = self.cls_token.expand(batch_size, -1, -1)
        # æ·»åŠ åˆ°åºåˆ—å¼€å¤´
        x = torch.cat([cls_tokens, x], dim=1)
        # æå–[CLS]æ ‡è®°è¾“å‡º
        cls_out = x[:, 0, :]
        # äº¤æ˜“æ¬ºè¯ˆé¢„æµ‹
        return self.classifier(cls_out)

    # åˆå§‹åŒ–æ¨¡å‹
    model = FraudTransformer(
        num_numerical_features=X_train_num[0].shape[1],
        categorical_dims=[100, 50, 20]
    )

    # å®šä¹‰æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
    criterion = nn.BCEWithLogitsLoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)

    # è®­ç»ƒæ¨¡å‹
    train_model(model, train_loader, val_loader, criterion, optimizer, device, num_epochs=10)

    # åŠ è½½æœ€ä½³æ¨¡å‹è¿›è¡Œæµ‹è¯•
    test_preds, test_labels = test_model(model, test_loader, device)
















# 7. ä¿å­˜æ¨¡å‹ï¼ˆä¿®æ”¹ï¼šåˆ›å»ºç›®å½•å¹¶ä¿å­˜ï¼‰
if not os.path.exists('model'):
    os.makedirs('model')
joblib.dump(model, 'model/ieee_lgbm_model.pkl')

# æ–°å¢ï¼šä¿å­˜ç±»åˆ«ç¼–ç å™¨å’Œç‰¹å¾åˆ—è¡¨
joblib.dump(cat_encoders, 'model/cat_encoders.pkl')  # ä¿å­˜ç±»åˆ«ç¼–ç å™¨
joblib.dump(X.columns.tolist(), 'model/feature_list.pkl')  # ä¿å­˜ç‰¹å¾åˆ—è¡¨
print("âœ… å·²ä¿å­˜é¢„å¤„ç†å·¥å…·: cat_encoders.pkl å’Œ feature_list.pkl")

# 8. æ¨¡å‹è¯„ä¼°
y_pred = model.predict(X_test)
y_prob = model.predict_proba(X_test)[:, 1]

print("ğŸ” Classification Report:")
print(classification_report(y_test, y_pred, digits=4))

print("ğŸ¯ ROC AUC Score:", roc_auc_score(y_test, y_prob))

# 9. æ··æ·†çŸ©é˜µå¯è§†åŒ–
cm = confusion_matrix(y_test, y_pred)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Not Fraud', 'Fraud'])
disp.plot(cmap='Blues')
plt.title('Confusion Matrix')
plt.grid(False)
plt.show()

# 10. ç²¾ç¡®ç‡ã€å¬å›ç‡ã€F1-score çƒ­åŠ›å›¾å±•ç¤º
report = classification_report(y_test, y_pred, output_dict=True)
metrics_df = pd.DataFrame(report).transpose().loc[['0', '1'], ['precision', 'recall', 'f1-score']]

plt.figure(figsize=(6, 4))
sns.heatmap(metrics_df, annot=True, cmap='YlGnBu', fmt=".4f")
plt.title('Precision, Recall, F1-score by Class')
plt.ylabel('Class')
plt.xlabel('Metric')
plt.tight_layout()
plt.show()

# 11. ç‰¹å¾é‡è¦æ€§å›¾
lgb.plot_importance(model, max_num_features=20, importance_type='gain')
plt.title("Top 20 Feature Importances")
plt.tight_layout()
plt.show()